{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Self-training\n","\n","The execution of this notebook denotes one iteration in the self-training process.\n","\n","The oracle is assumed to be already trained an stored in `models/oracle`.\n","\n","Firstly a u-net is trained using the supervised data in the first iteration.\n","\n","The trained segmenter computes segmentation for unsupervised data. \n","\n","The oracle determines the highly confident predictions (either in class 5 or 0 in the `buckets_classification` modality).\n","\n","A sampling using the size of the computed liver is performed. All selected samples are inserted into the training data (see last line).\n","\n","Re-execute the notebook to carry out a new iteration."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"import h5py, random\nimport tensorflow as tf\nimport numpy as np\nfrom libs.generators.utils import get_case_length, get_x_slice, get_y_slice\nfrom libs.models.u_net import get_model_unet\nfrom libs.postprocessing import self_ensembling"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"config = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nsession = tf.Session(config=config)"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = get_model_unet(input_size =(416, 416, 1), feature_maps = 16, output_layers=1, output_type='sigmoid')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from libs.generators.ssl_batch_generator import SemiSupervisedBatchGenerator\n","\n","gen = SemiSupervisedBatchGenerator()"]},{"cell_type":"markdown","metadata":{},"source":["## Training"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"from keras.optimizers import Adam\nfrom libs.metrics import dice_coef_sig\n\nmodel.compile(optimizer= Adam(lr = 1e-4, clipnorm = 1., clipvalue = 0.5), loss = 'binary_crossentropy', metrics = [ dice_coef_sig, 'binary_accuracy'])"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"model.fit_generator(gen, epochs = 120, verbose= 1, workers = 32, max_queue_size = 100)"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"model.compile(optimizer= Adam(lr = 1e-5, clipnorm = 1., clipvalue = 0.5), loss = 'binary_crossentropy', metrics = [dice_coef_sig, 'binary_accuracy'])"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"model.fit_generator(gen, epochs = 30, verbose = 1, workers = 32, max_queue_size = 100)"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from libs.keras_checkpoints import load_model\n","\n","oracle = load_model('models/oracle')"]},{"cell_type":"markdown","metadata":{},"source":["## Inference and collection of highly confident data by the oracle"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"def get_unsupervised_samples(supervised= 0):\n    '''Getter of unsupervised data indices.\n    @param supervised The id of the supervised volume\n    @return indices of unsupervised data\n    '''\n    indices = []\n    with h5py.File('data/training_data.h5', 'r') as hdf:\n        for i in range(0, 20):\n            if i == supervised:\n                continue\n            l = get_case_length(hdf, i)\n            for j in range(l):\n                indices.append((i, j))\n        return indices"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["indices = get_unsupervised_samples()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def batch_normalization(x):\n","    return (x - x.mean()) / x.std()\n","\n","def stack_segmentation_rgb(x, y):\n","    b = np.full(x.shape, x.mean(), 'float32')\n","    g = x + y.reshape(416, 416, 1)\n","    return batch_normalization(np.concatenate((x, g, b), axis= -1))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"collected_high_iou = []\ncollected_undefined_iou = []\n\nwith h5py.File('data/training_data.h5', 'r') as hdf:\n    print('Start evaluating unlabeled data')\n    for i in indices:\n        x = get_x_slice(hdf, index[0], index[1])\n        P = self_ensembling(x, model)\n        rgb = stack_segmentation_rgb(x, p)\n        oracle_verdict = np.argmax(oracle.predict(rgb.reshape(1, 416, 416, 3))[0])\n        if oracle_verdict == 5: # in P_{high}\n            collected_high_iou.append((index[0], index[1], P, P.sum()))\n        if oracle_verdict == 0 and P.sum() == 0: # in P_{nan}\n            collected_undefined_iou.append((index[0], index[1], P, P.sum()))"},{"cell_type":"markdown","metadata":{},"source":["## Sampling using the size of the predicted livers"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"def sampling(collected_data, generator, liver_area = 35000):\n    tau = 50 # min data to add at each iteration\n    \n    Pn = 0\n    while Pn < tau and 0 < liver_area :\n        liver_area = liver_area - 1000\n        sampled = filter(lambda x: x[3] > t, collected_data)\n        Pn = generator.count_new_samples(sampled)\n    return sampled"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"high_confident_data = sampling(collected_high_iou, gen, 35000)\nrandom.shuffle(collected_undefined_iou)\nsampled_nan = collected_undefined_iou[:len(high_confident_data)]"},{"cell_type":"markdown","metadata":{},"source":["## Add data in semi-supervised dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["gen.add_separate_samples(high_confident_data, sampled_nan)"]},{"cell_type":"markdown","metadata":{},"source":["## Restart the process"]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}